---
title: "Homework 6"
author: "Connie Zhang"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(mgcv)
library(p8105.datasets)
library(patchwork)
```

## Question 1
```{r - Question 1 - data cleaning}
birth_data = read_csv("./data/birthweight.csv") %>% 
janitor::clean_names() %>%
mutate (
  babysex = recode(babysex, "1" = "male", "2" = "female"),
  frace = recode(frace,
                        "1" = "white",
                        "2" = "black",
                        "3" = "asian",
                        "4" = "puetro rican",
                        "8" = "other",
                        "9" = "unknown"),
         mrace = recode(mrace,
                        "1" = "white",
                        "2" = "black",
                        "3" = "asian",
                        "4" = "puetro rican",
                        "8" = "other"),
           malform = recode(malform, 
                          "0" = "absent",
                          "1" = "present"
                          ))  
birth_data


#checking for missing values 
sum(!complete.cases(birth_data))

```

* There are no missing values in the data, as shown by the `0` value output. 

```{r - regression modeling}
birthmodel_fit = lm(bwt ~ 
                      gaweeks + 
                      delwt + 
                      fincome +
                      bhead +
                      blength +
                      smoken, 
                    data = birth_data) 

summary(birthmodel_fit) 
```

* These variables were chosen as well-known and regarded predictors of birthweight for newborns, including both biological, lifestyle, and socioeconomic factors. 
```{r - fitted vs residual}
birth_data %>%
  
  modelr::add_residuals(birthmodel_fit) %>%
  modelr::add_predictions(birthmodel_fit) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point() + geom_hline(yintercept = 0, color = "blue") 
  labs(
        title = "Residuals vs. Predicted Values on a Hypothesized Model")

```

* The majority of residuals fall around the value of 0 and show constant variance. A few are shown as extremely high or low residuals, which could be outliers or data entry errors.  

```{r - comparison models}
model_2 = lm(bwt ~ blength + gaweeks, data = birth_data) 

model_3 = lm(bwt ~ bhead + babysex + blength + bhead * babysex * blength, data = birth_data) 
```

```{r}
comp_data = 
  crossv_mc(birth_data, 100) %>%

  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) 

comp_data = 
  comp_data %>% 
  mutate(newbirthmodel_fit  = map(train, ~lm(bwt ~ 
                      gaweeks + 
                      delwt + 
                      fincome + 
                      bhead + 
                      blength +
                      smoken, data = .x)),
         new_model_2    = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         new_model_3  = map(train, ~gam(bwt ~ bhead + babysex + blength + bhead * babysex * blength, data = as_tibble(.x)))) %>% 
  
  mutate(rmse_birthmodel = map2_dbl(newbirthmodel_fit, test, ~rmse(model = .x, data = .y)),
         rmse_model_2    = map2_dbl(new_model_2, test, ~rmse(model = .x, data = .y)),
         rmse_model_3 = map2_dbl(new_model_3, test, ~rmse(model = .x, data = .y))) 
```

```{r}
comp_data %>%
select(starts_with("rmse")) %>% 
  pivot_longer(
      everything(),
      names_to = "model", 
      values_to = "rmse",
      names_prefix = "rmse_") %>% 
    mutate(model = fct_inorder(model)) %>% 
    ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_violin() + 
   scale_x_discrete(labels=c("birthmodel" = "Initial", 
                            "model_2" = "Comparison 2 ",
                            "model_3" = "Comparison 3")) +
  labs(
    title = "Distribution of Prediction Error Across Models",
    x = "Model",
    y = "RMSE"
  )

```

* Through comparison across models, the initial model created (including gestational age, delivery weight of mother, financial income of mother, baby's length and head size, as well as number of cigarettes smoked by mother) has the lowest RMSE. This indicates that it is the strongest model.


## Question 2

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
#for each bootstrap sample, produce estimates of these two quantities
boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, sample_frac(weather_df, replace = TRUE)))

 bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    results_r = map(models, broom::glance),
    results_estimate = map(models, broom::tidy)) %>% 
  select(-strap_sample, -models) %>% 
  unnest() %>%
  select(strap_number, r.squared, term, estimate) %>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>%
  janitor::clean_names() %>%
  mutate(log_betaest = log(intercept * tmin)) 
 
bootstrap_results %>%
  summarise("r_square" = mean(r_squared),
  "log(b0 *b1)" = mean(log_betaest)) %>%
  knitr::kable(digits = 3)
   
```

```{r - plot distribution}
#plotting the distribution of estimates
distribution_1 = ggplot(data = bootstrap_results, aes(x = r_squared)) +
  geom_histogram() +
  labs(title = "R-squared Estimates",x = "r squared estimates",
    y = "count")

distribution_2 = ggplot(data = bootstrap_results, aes(x = log_betaest)) +
  geom_histogram() +
  labs(title = "Log Product Coefficient Estimates",x = "log estimates",
    y = "count")

distribution_1 + distribution_2
```

* For the distribution of r-squared estimates, approximately ~88% to ~93% of the variance can be explained in the model. We can interpret this as each bootstrap model explains 88%-93% of the variability in birthweight as an outcome. The product of log estimates behave similarly with a fairly normal distribution in the range of values. 

```{r}
#identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r-squared
quantile(pull(bootstrap_results, r_squared), probs = c(0.025, 0.975))
```

```{r}
#identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for log(b1-b0) 
quantile(pull(bootstrap_results, log_betaest), probs = c(0.025, 0.975))
```
